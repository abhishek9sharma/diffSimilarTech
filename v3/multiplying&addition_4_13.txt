operation, boilerplate, complex, ---------------------------------------------------

complex,operation
however multiplying is a more complex operation than addition or shifting

boilerplate,complex
the boilerplate code would multiplying rapidly when the express get more complex than addition of two terms

others---------------------------------------------------

instance,larger,value,exponent,smaller,largerexp-smallerexp,values
for instance to achieve addition you would scale the larger value to have the same exponent as the smaller one by multiplying it by 10 largerexp-smallerexp then adding the two values and rescaling

exact,behavior,better,sum,result
so the compiler can t make the optimization because it can t tell if you wanted the exact behavior where multiplying is better or the implemented behavior where the scale of sum affects the result of the addition

cycles,faster,multiplication
in the remote case those operations are not simplified assuming that there is a jit that maps the multiplication and add opcodes in a 1 1 relationship to their cpu instruction counterparts in most modern architectures all integer arithmetic operations usually take the same number of cycles so it will be faster multiplying once than add four times just checked it addition is still slightly faster than multiplication 1 clock vs 3 clocks so it still pays using a multiplication here

time,step
functionally a multiplying will always take more time than an add because it combines a true multiplying along with a true addition step

numeric,expression,multiplication,higher,precedence,subtraction
either way your example with the numeric expression would multiplying by 3 first because multiplication has higher precedence than addition or subtraction

slower
if multiplying is slower than addition then case 2 is slightly slower than case 1

x1,faster,todays,cpus
you can try x1 c1 and then x1 + c1 but i don t think the addition is much faster than multiplying on todays cpus

slower,simple,temporary,register
we don t actually multiplying it s slower than simple addition and as you can see we destroy temporary register t0 but don t touch s0 s1

num2,function,better
since you want to do same thing couple times writing your multiplication num1 num2 function yourself with only addition and using that when multiplying would be much better choice

gradeschool,circuit,deeper
so we must show that a gradeschool multiplying circuit is o log n times deeper than an addition circuit

fma,longer
you didn t provide a full code sample that includes the surrounding loop presumably there is a surrounding loop so it is hard to answer definitively but the main problem i see is that the latency of the dependency chains of your fma code is considerably longer than your multiplying + addition code

