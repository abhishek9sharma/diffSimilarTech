others---------------------------------------------------

characters,bigrams,compression,algorithm,occurrences,shorter,delta
if some characters or bigrams or n-gram occur more frequently a compression algorithm can encode this distribution skew to codes that favor the frequent occurrences with shorter code words and you get a delta of compression

collections.counter,subsequent,combinations,frequency,count,print,value
i tried using collections.counter to get the each subsequent words combinations frequency count and print all n-gram that come up more than 2 times sorted by value

suggestion,bad,general,fractions
arun s suggestion is not bad but the n-gram alone are more geared to find general fractions of words

words.,character,statistics,easier,text,typos,tend,spelling
the problem is that you re not going to find appropriate statistics for all possible words. character n-gram statistics are easier to accumulate and more robust even for text without typos words in a language tend to follow the same spelling patterns

support,ideas
it has support for tokenizing lemmatizing n-gram ideas that span more than one words

handier,google,phrase,output,probability,likely
furthermore it is handier than google n-gram as for a given phrase it does not simply output its absolute frequency but it can output its joint probability conditional probability and even the most likely words that follow

character,lower,test,error,features
shouldn t the character n-gram have a lower test error since it extracted more features than the words features

text,functional,bag,pruning,hapaxes,approximate,better
if you will be classifying multi-paragraph text all in one language a functional words list which your bag of words with pruning of hapaxes will quickly approximate might well serve you perfectly and could work better than n-gram

character,level,better,logistic
for some problems character level n-gram do better than words level and logistic regression parameters

